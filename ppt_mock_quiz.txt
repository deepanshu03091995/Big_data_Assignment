**1. Write an SQL query to find the second-highest salary from an "Employees" table.**

SELECT salary
FROM Employees
ORDER BY salary DESC
LIMIT 2
OFFSET 1;

**2. Write a MapReduce program to calculate the word count of a given input text file.**

import sys

def mapper(key, value):
    for word in value.split():
        yield word, 1

def reducer(key, values):
    count = sum(values)
    yield key, count

if __name__ == "__main__":
    input_file = sys.argv[1]
    output_file = sys.argv[2]

    with open(input_file, "r") as f:
        for line in f:
            for word, count in mapper(None, line):
                yield word, count

    with open(output_file, "w") as f:
        for key, count in reducer(None, []):
            f.write("%s\t%d\n" % (key, count))

**3. Write a Spark program to count the number of occurrences of each word in a given text file.**
import pyspark.sql.functions as F

def word_count(text_file):

    spark = pyspark.sql.SparkSession.builder.appName("Word Count").getOrCreate()

    # Read the text file into a Spark DataFrame.
    df = spark.read.text(text_file)

    # Split the DataFrame into words.
    words = df.select(F.explode(F.split(df["value"], " "))).toDF("word")

    # Count the number of occurrences of each word.
    word_counts = words.groupBy("word").count()

    # Print the word counts.
    for word, count in word_counts.collect():
        print(f"{word}: {count}")

if __name__ == "__main__":

    text_file = "/path/to/text/file"

    word_count(text_file)
            
