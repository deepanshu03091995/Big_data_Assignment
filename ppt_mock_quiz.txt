**1. Write an SQL query to find the second-highest salary from an "Employees" table.**

SELECT salary
FROM Employees
ORDER BY salary DESC
LIMIT 2
OFFSET 1;

**2. Write a MapReduce program to calculate the word count of a given input text file.**

import sys

def mapper(key, value):
    for word in value.split():
        yield word, 1

def reducer(key, values):
    count = sum(values)
    yield key, count

if __name__ == "__main__":
    input_file = sys.argv[1]
    output_file = sys.argv[2]

    with open(input_file, "r") as f:
        for line in f:
            for word, count in mapper(None, line):
                yield word, count

    with open(output_file, "w") as f:
        for key, count in reducer(None, []):
            f.write("%s\t%d\n" % (key, count))

**3. Write a Spark program to count the number of occurrences of each word in a given text file.**
import pyspark

def word_count(text_file):

    # Create a SparkContext.

    sc = pyspark.SparkContext()

    # Read the text file into a Spark RDD.

    rdd = sc.textFile(text_file)

    # Split the RDD into words.

    words = rdd.flatMap(lambda line: line.split(" "))

    # Count the number of occurrences of each word.

    word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)

    # Print the word counts.

    for word, count in word_counts.collect():
        print(f"{word}: {count}")

if __name__ == "__main__":

    text_file = "/path/to/text/file"

    word_count(text_file)

            

            
