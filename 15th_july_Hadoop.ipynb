{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1123b8",
   "metadata": {},
   "source": [
    "# 1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c54f4d2",
   "metadata": {},
   "source": [
    "import configparser\n",
    "\n",
    "def read_hadoop_config(file_path):\n",
    "    # Create a ConfigParser object\n",
    "    config = configparser.ConfigParser()\n",
    "\n",
    "    # Read the configuration file\n",
    "    config.read(file_path)\n",
    "\n",
    "    # Get the sections in the configuration file\n",
    "    sections = config.sections()\n",
    "\n",
    "    # Display the core components\n",
    "    if 'core-site' in sections:\n",
    "        print(\"Core Components:\")\n",
    "        print(\"----------------\")\n",
    "        for key, value in config.items('core-site'):\n",
    "            print(f\"{key} = {value}\")\n",
    "        print(\"----------------\")\n",
    "    else:\n",
    "        print(\"No core components found in the configuration file.\")\n",
    "\n",
    "    # You can add similar code for other components like 'hdfs-site', 'yarn-site', etc.\n",
    "\n",
    "    # Specify the path to your Hadoop configuration file\n",
    "file_path = '/path/to/hadoop/conf/core-site.xml'\n",
    "\n",
    "    # Call the function to read and display the core components\n",
    "read_hadoop_config(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03f68b",
   "metadata": {},
   "source": [
    "# 2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb780ca",
   "metadata": {},
   "source": [
    "from pywebhdfs.webhdfs import PyWebHdfsClient\n",
    "\n",
    "def calculate_total_file_size(hdfs_host, hdfs_port, hdfs_user, hdfs_directory):\n",
    "    # Create a PyWebHdfsClient object\n",
    "    hdfs = PyWebHdfsClient(host=hdfs_host, port=hdfs_port, user_name=hdfs_user)\n",
    "\n",
    "    # Get the file status for the HDFS directory\n",
    "    directory_status = hdfs.list_status(hdfs_directory)\n",
    "\n",
    "    total_size = 0\n",
    "\n",
    "    # Iterate over the file status entries\n",
    "    for entry in directory_status['FileStatuses']['FileStatus']:\n",
    "        if entry['type'] == 'FILE':\n",
    "            total_size += entry['length']\n",
    "\n",
    "    return total_size\n",
    "\n",
    "    # Specify the HDFS host, port, user, and directory path\n",
    "hdfs_host = 'localhost'\n",
    "hdfs_port = 50070\n",
    "hdfs_user = 'hadoop'\n",
    "hdfs_directory = '/user/hadoop/data'\n",
    "\n",
    "    # Call the function to calculate the total file size\n",
    "total_size = calculate_total_file_size(hdfs_host, hdfs_port, hdfs_user, hdfs_directory)\n",
    "\n",
    "    # Display the total file size in bytes\n",
    "print(f\"Total file size in {hdfs_directory}: {total_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f37a403",
   "metadata": {},
   "source": [
    "# 3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4975014a",
   "metadata": {},
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(TopNWords, self).configure_args()\n",
    "        self.add_passthru_arg('--topN', type=int, default=10, help='Number of top words to display')\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_top_words)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        words = WORD_RE.findall(line)\n",
    "        for word in words:\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    def reducer_find_top_words(self, _, word_count_pairs):\n",
    "        topN = self.options.topN\n",
    "        sorted_word_count_pairs = sorted(word_count_pairs, reverse=True)\n",
    "        for i in range(topN):\n",
    "            count, word = sorted_word_count_pairs[i]\n",
    "            yield f'Top {i+1}', (word, count)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TopNWords.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd796129",
   "metadata": {},
   "source": [
    "# 4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e13465",
   "metadata": {},
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def check_hadoop_health(namenode_host, namenode_port):\n",
    "    # Check NameNode health status\n",
    "    \n",
    "    namenode_url = f\"http://{namenode_host}:{namenode_port}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\"\n",
    "    try:\n",
    "        response = requests.get(namenode_url)\n",
    "        response.raise_for_status()\n",
    "        namenode_status = json.loads(response.text)['beans'][0]['State']\n",
    "        print(\"NameNode Health Status:\", namenode_status)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error occurred while checking NameNode health status:\", str(e))\n",
    "\n",
    "    # Check DataNode health status\n",
    "    \n",
    "    datanode_url = f\"http://{namenode_host}:{namenode_port}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
    "    try:\n",
    "        response = requests.get(datanode_url)\n",
    "        response.raise_for_status()\n",
    "        datanodes = json.loads(response.text)['beans']\n",
    "        for datanode in datanodes:\n",
    "            datanode_status = datanode['State']\n",
    "            datanode_host = datanode['Host']\n",
    "            print(f\"DataNode Health Status ({datanode_host}):\", datanode_status)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error occurred while checking DataNode health status:\", str(e))\n",
    "\n",
    "    # Specify the NameNode host and port\n",
    "namenode_host = 'localhost'\n",
    "namenode_port = 9870\n",
    "\n",
    "    # Call the function to check Hadoop health status\n",
    "check_hadoop_health(namenode_host, namenode_port)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72dfd5c",
   "metadata": {},
   "source": [
    "# 5. Develop a Python program that lists all the files and directories in a specific HDFS path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f002401b",
   "metadata": {},
   "source": [
    "from pywebhdfs.webhdfs import PyWebHdfsClient\n",
    "\n",
    "def list_hdfs_path(hdfs_host, hdfs_port, hdfs_user, hdfs_path):\n",
    "    # Create a PyWebHdfsClient object\n",
    "    hdfs = PyWebHdfsClient(host=hdfs_host, port=hdfs_port, user_name=hdfs_user)\n",
    "\n",
    "    # List files and directories in the specified HDFS path\n",
    "    listing = hdfs.list_dir(hdfs_path)['FileStatuses']['FileStatus']\n",
    "\n",
    "    # Print the listing\n",
    "    for entry in listing:\n",
    "        name = entry['pathSuffix']\n",
    "        is_directory = entry['type'] == 'DIRECTORY'\n",
    "        print(f\"{name} {'(directory)' if is_directory else ''}\")\n",
    "\n",
    "    # Specify the HDFS host, port, user, and path\n",
    "hdfs_host = 'localhost'\n",
    "hdfs_port = 50070\n",
    "hdfs_user = 'hadoop'\n",
    "hdfs_path = '/user/hadoop/data'\n",
    "\n",
    "    # Call the function to list files and directories\n",
    "list_hdfs_path(hdfs_host, hdfs_port, hdfs_user, hdfs_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e224f",
   "metadata": {},
   "source": [
    "# 6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47210116",
   "metadata": {},
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def analyze_storage_utilization(namenode_host, namenode_port):\n",
    "    # Get the DataNodes information\n",
    "    datanode_url = f\"http://{namenode_host}:{namenode_port}/jmx?qry=Hadoop:service=NameNode,name=DataNodeInfo\"\n",
    "    try:\n",
    "        response = requests.get(datanode_url)\n",
    "        response.raise_for_status()\n",
    "        datanodes = json.loads(response.text)['beans']\n",
    "        \n",
    "        # Sort DataNodes by storage capacity in descending order\n",
    "        sorted_datanodes = sorted(datanodes, key=lambda d: d['Capacity'], reverse=True)\n",
    "\n",
    "        # Get the DataNode with the highest storage capacity\n",
    "        highest_datanode = sorted_datanodes[0]\n",
    "        highest_datanode_host = highest_datanode['Host']\n",
    "        highest_datanode_capacity = highest_datanode['Capacity']\n",
    "\n",
    "        # Get the DataNode with the lowest storage capacity\n",
    "        lowest_datanode = sorted_datanodes[-1]\n",
    "        lowest_datanode_host = lowest_datanode['Host']\n",
    "        lowest_datanode_capacity = lowest_datanode['Capacity']\n",
    "\n",
    "        print(\"DataNode Storage Utilization Analysis:\")\n",
    "        print(\"--------------------------------------\")\n",
    "        print(f\"Highest Capacity: {highest_datanode_host} - {highest_datanode_capacity} bytes\")\n",
    "        print(f\"Lowest Capacity: {lowest_datanode_host} - {lowest_datanode_capacity} bytes\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error occurred while analyzing storage utilization:\", str(e))\n",
    "\n",
    "    # Specify the NameNode host and port\n",
    "namenode_host = 'localhost'\n",
    "namenode_port = 9870\n",
    "\n",
    "    # Call the function to analyze storage utilization\n",
    "analyze_storage_utilization(namenode_host, namenode_port)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71a8410",
   "metadata": {},
   "source": [
    "# 7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514129f6",
   "metadata": {},
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(resourcemanager_host, resourcemanager_port, job_properties):\n",
    "\n",
    "    # Submit the Hadoop job\n",
    "    \n",
    "    submit_url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/new-application\"\n",
    "    try:\n",
    "        response = requests.post(submit_url)\n",
    "        response.raise_for_status()\n",
    "        application_id = json.loads(response.text)['application-id']\n",
    "        print(\"Submitted Hadoop job. Application ID:\", application_id)\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error occurred while submitting Hadoop job:\", str(e))\n",
    "        return None\n",
    "\n",
    "    # Set the job properties\n",
    "    \n",
    "    job_properties['application-id'] = application_id\n",
    "\n",
    "    # Submit the job properties to start the job\n",
    "    \n",
    "    submit_job_url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(submit_job_url, json=job_properties)\n",
    "        response.raise_for_status()\n",
    "        print(\"Hadoop job submitted successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error occurred while submitting Hadoop job properties:\", str(e))\n",
    "        return None\n",
    "\n",
    "    return application_id\n",
    "\n",
    "def monitor_job_progress(resourcemanager_host, resourcemanager_port, application_id):\n",
    "    # Monitor the job progress\n",
    "    \n",
    "    status_url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/{application_id}\"\n",
    "    try:\n",
    "        while True:\n",
    "            response = requests.get(status_url)\n",
    "            response.raise_for_status()\n",
    "            status = json.loads(response.text)['app']['state']\n",
    "            print(\"Job status:\", status)\n",
    "            \n",
    "            if status == \"FINISHED\" or status == \"FAILED\" or status == \"KILLED\":\n",
    "                break\n",
    "\n",
    "            time.sleep(5)  # Wait for 5 seconds before checking the status again\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error occurred while monitoring job progress:\", str(e))\n",
    "\n",
    "def retrieve_job_output(resourcemanager_host, resourcemanager_port, application_id):\n",
    "    # Retrieve the job output\n",
    "    \n",
    "    output_url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/{application_id}/state\"\n",
    "    try:\n",
    "        response = requests.get(output_url)\n",
    "        response.raise_for_status()\n",
    "        output = json.loads(response.text)['app']['finalStatus']\n",
    "        print(\"Job output:\", output)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error occurred while retrieving job output:\", str(e))\n",
    "\n",
    "    # Specify the ResourceManager host and port\n",
    "resourcemanager_host = 'localhost'\n",
    "resourcemanager_port = 8088\n",
    "\n",
    "    # Specify the job properties\n",
    "job_properties = {\n",
    "    \"application-name\": \"MyHadoopJob\",\n",
    "    \"application-type\": \"MAPREDUCE\",\n",
    "    \"am-container-spec\": {\n",
    "        \"commands\": {\n",
    "            \"command\": \"hadoop jar /path/to/hadoop-job.jar input output\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "    #   Submit the Hadoop job and get the application ID\n",
    "application_id = submit_hadoop_job(resourcemanager_host, resourcemanager_port, job_properties)\n",
    "\n",
    "    # Monitor the job progress\n",
    "if application_id:\n",
    "    monitor_job_progress(resourcemanager_host, resourcemanager_port, application_id)\n",
    "\n",
    "    # Retrieve the job output\n",
    "    retrieve_job_output(resourcemanager_host, resourcemanager_port, application_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb12fc",
   "metadata": {},
   "source": [
    "# 8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd64a23",
   "metadata": {},
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(resourcemanager_host, resourcemanager_port, job_properties):\n",
    "\n",
    "    # Submit the Hadoop job\n",
    "    \n",
    "    submit_url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/new-application\"\n",
    "    try:\n",
    "        response = requests.post(submit_url)\n",
    "        response.raise_for_status()\n",
    "        application_id = json.loads(response.text)['application-id']\n",
    "        print(\"Submitted Hadoop job. Application ID:\", application_id)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error occurred while submitting Hadoop job:\", str(e))\n",
    "        return None\n",
    "\n",
    "    # Set the job properties\n",
    "    job_properties['application-id'] = application_id\n",
    "\n",
    "    # Submit the job properties to start the job\n",
    "    submit_job_url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps\"\n",
    "    try:\n",
    "        response = requests.post(submit_job_url, json=job_properties)\n",
    "        response.raise_for_status()\n",
    "        print(\"Hadoop job submitted successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error occurred while submitting Hadoop job properties:\", str(e))\n",
    "        return None\n",
    "\n",
    "    return application_id\n",
    "\n",
    "def set_resource_requirements(resourcemanager_host, resourcemanager_port, application_id, vcores, memory):\n",
    "\n",
    "    # Set the resource requirements for the Hadoop job\n",
    "    \n",
    "    resource_url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/{application_id}\"\n",
    "    try:\n",
    "        response = requests.get(resource_url)\n",
    "        response.raise_for_status()\n",
    "        app = json.loads(response.text)['app']\n",
    "        app['am-container-spec']['resource']['vCores'] = vcores\n",
    "        app['am-container-spec']['resource']['memoryMB'] = memory\n",
    "\n",
    "        response = requests.put(resource_url, json=app)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        print(\"Resource requirements set successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error occurred while setting resource requirements:\", str(e))\n",
    "\n",
    "def track_resource_usage(resourcemanager_host, resourcemanager_port, application_id):\n",
    "\n",
    "    # Track the resource usage during job execution\n",
    "    \n",
    "    track_url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/{application_id}/appattempts\"\n",
    "    try:\n",
    "        while True:\n",
    "            response = requests.get(track_url)\n",
    "            response.raise_for_status()\n",
    "            app_attempts = json.loads(response.text)['appAttempts']\n",
    "\n",
    "            if len(app_attempts) == 0:\n",
    "                print(\"No app attempts found.\")\n",
    "                break\n",
    "\n",
    "            latest_attempt_id = app_attempts[-1]['appAttemptId']\n",
    "\n",
    "            attempt_url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/{application_id}/appattempts/{latest_attempt_id}\"\n",
    "            response = requests.get(attempt_url)\n",
    "            response.raise_for_status()\n",
    "            attempt = json.loads(response.text)['appAttempt']\n",
    "\n",
    "            if 'trackingUrl' in attempt:\n",
    "                tracking_url = attempt['trackingUrl']\n",
    "                print(\"Tracking URL:\", tracking_url)\n",
    "            \n",
    "            if attempt['appAttemptState'] in ['FINISHED', 'FAILED', 'KILLED']:\n",
    "                break\n",
    "\n",
    "            time.sleep(5)  # Wait for 5 seconds before checking the status again\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error occurred while tracking resource usage:\", str(e))\n",
    "\n",
    "    # Specify the ResourceManager host and port\n",
    "    \n",
    "resourcemanager_host = 'localhost'\n",
    "resourcemanager_port = 8088\n",
    "\n",
    "    # Specify the job properties\n",
    "    \n",
    "job_properties = {\n",
    "    \"application-name\": \"MyHadoopJob\",\n",
    "    \"application-type\": \"MAPREDUCE\",\n",
    "    \"am-container-spec\": {\n",
    "        \"commands\": {\n",
    "            \"command\": \"hadoop jar /path/to/hadoop-job.jar input output\"\n",
    "        },\n",
    "        \"resource\": {\n",
    "            \"vCores\": 2,  # Specify the desired number of vCores\n",
    "            \"memoryMB\": 4096  # Specify the desired amount of memory in MB\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "    # Submit the Hadoop job and get the application ID\n",
    "    \n",
    "application_id = submit_hadoop_job(resourcemanager_host, resourcemanager_port, job_properties)\n",
    "\n",
    "    # Set the resource requirements\n",
    "    \n",
    "if application_id:\n",
    "\n",
    "    vcores = job_properties['am-container-spec']['resource']['vCores']\n",
    "    memory = job_properties['am-container-spec']['resource']['memoryMB']\n",
    "    set_resource_requirements(resourcemanager_host, resourcemanager_port, application_id, vcores, memory)\n",
    "\n",
    "    # Track the resource usage\n",
    "    track_resource_usage(resourcemanager_host, resourcemanager_port, application_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66366586",
   "metadata": {},
   "source": [
    "# 9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ece67",
   "metadata": {},
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "\n",
    "class MapReduceJob(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(MapReduceJob, self).configure_args()\n",
    "        self.add_passthru_arg('--split-size', type=int, default=100, help='Input split size in MB')\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   reducer=self.reducer)\n",
    "        ]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Your mapper logic here\n",
    "        \n",
    "        yield None, line\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        # Your reducer logic here\n",
    "        \n",
    "        for value in values:\n",
    "            yield key, value\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Specify the input file path\n",
    "    \n",
    "    input_file = '/path/to/input_file.txt'\n",
    "\n",
    "    # Specify the input split size (in MB)\n",
    "    \n",
    "    split_size = 100\n",
    "\n",
    "    # Calculate the number of bytes per split\n",
    "    \n",
    "    bytes_per_split = split_size * 1024 * 1024\n",
    "\n",
    "    # Get the file size\n",
    "    \n",
    "    file_size = os.path.getsize(input_file)\n",
    "\n",
    "    # Calculate the number of splits\n",
    "    \n",
    "    num_splits = file_size // bytes_per_split\n",
    "\n",
    "    # Create an instance of the MapReduce job\n",
    "    \n",
    "    mr_job = MapReduceJob(args=[input_file])\n",
    "\n",
    "    # Run the MapReduce job for each split\n",
    "    \n",
    "    for i in range(num_splits + 1):\n",
    "        start_byte = i * bytes_per_split\n",
    "        end_byte = start_byte + bytes_per_split\n",
    "\n",
    "        # Set the input protocol property to control the split size\n",
    "        mr_job.options.input_protocol = 'raw_value:{},{}'.format(start_byte, end_byte)\n",
    "\n",
    "        # Run the MapReduce job\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Execution Time:\", execution_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330251d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
