{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9347277c",
   "metadata": {},
   "source": [
    "# TOPIC: Docker\n",
    "1. Scenario: You are building a microservices-based application using Docker. Design a Docker Compose file that sets up three containers: a web server container, a database container, and a cache container. Ensure that the containers can communicate with each other properly.\n",
    "2. Scenario: You want to scale your Docker containers dynamically based on the incoming traffic. Write a Python script that utilizes Docker SDK to monitor the CPU usage of a container and automatically scales the number of replicas based on a threshold.\n",
    "3. Scenario: You have a Docker image stored on a private registry. Develop a script in Bash that authenticates with the registry, pulls the latest version of the image, and runs a container based on that image.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b5ea4f",
   "metadata": {},
   "source": [
    "**1. Scenario: You are building a microservices-based application using Docker. Design a Docker Compose file that sets up three containers: a web server container, a database container, and a cache container. Ensure that the containers can communicate with each other properly.**\n",
    "\n",
    "    version: '3.8'\n",
    "    services:\n",
    "      web:\n",
    "        image: nginx\n",
    "        ports:\n",
    "          - 80:80\n",
    "        depends_on:\n",
    "          - db\n",
    "          - cache\n",
    "      db:\n",
    "        image: mysql:5.7\n",
    "        environment:\n",
    "          MYSQL_ROOT_PASSWORD: password\n",
    "      cache:\n",
    "        image: redis:6.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e48444",
   "metadata": {},
   "source": [
    "**2. Scenario: You want to scale your Docker containers dynamically based on the incoming traffic. Write a Python script that utilizes Docker SDK to monitor the CPU usage of a container and automatically scales the number of replicas based on a threshold.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110051d9",
   "metadata": {},
   "source": [
    "    import docker\n",
    "\n",
    "    def scale_containers(cpu_threshold):\n",
    "      client = docker.from_env()\n",
    "      containers = client.containers.list()\n",
    "\n",
    "      for container in containers:\n",
    "        if container.cpu_percent > cpu_threshold:\n",
    "          container.scale(replicas=2)\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "      scale_containers(80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c2bba",
   "metadata": {},
   "source": [
    "**3. Scenario: You have a Docker image stored on a private registry. Develop a script in Bash that authenticates with the registry, pulls the latest version of the image, and runs a container based on that image.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461c088c",
   "metadata": {},
   "source": [
    "    #!/bin/bash\n",
    "\n",
    "    registry_url=\"https://my-registry.com\"\n",
    "    username=\"username\"\n",
    "    password=\"password\"\n",
    "    image_name=\"my-image\"\n",
    "\n",
    "    docker login $registry_url -u $username -p $password\n",
    "    docker pull $registry_url/$image_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac034213",
   "metadata": {},
   "source": [
    "# TOPIC: Airflow\n",
    "1. Scenario: You have a data pipeline that requires executing a shell command as part of a task. Create an Airflow DAG that includes a BashOperator to execute a specific shell command.\n",
    "2. Scenario: You want to create dynamic tasks in Airflow based on a list of inputs. Design an Airflow DAG that generates tasks dynamically using PythonOperator, where each task processes an element from the input list.\n",
    "3. Scenario: You need to set up a complex task dependency in Airflow, where Task B should start only if Task A has successfully completed. Implement this dependency using the \"TriggerDagRunOperator\" in Airflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e4101",
   "metadata": {},
   "source": [
    "**Scenario: You have a data pipeline that requires executing a shell command as part of a task. Create an Airflow DAG that includes a BashOperator to execute a specific shell command.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf648e78",
   "metadata": {},
   "source": [
    "    from airflow import DAG\n",
    "    from airflow.operators.bash import BashOperator\n",
    "\n",
    "    dag = DAG(\"shell_command_dag\", schedule_interval=\"@daily\")\n",
    "\n",
    "    t1 = BashOperator(\n",
    "        task_id=\"execute_shell_command\",\n",
    "        bash_command=\"echo 'Hello, world!'\",\n",
    "        dag=dag,\n",
    "    )\n",
    "\n",
    "    t1.set_upstream(None)\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        dag.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4f3cd8",
   "metadata": {},
   "source": [
    "**Scenario: You want to create dynamic tasks in Airflow based on a list of inputs. Design an Airflow DAG that generates tasks dynamically using PythonOperator, where each task processes an element from the input list.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841a7298",
   "metadata": {},
   "source": [
    "    from airflow import DAG\n",
    "    from airflow.operators.python import PythonOperator\n",
    "\n",
    "    def process_list_item(item):\n",
    "        print(f\"Processing item: {item}\")\n",
    "\n",
    "    dag = DAG(\"dynamic_task_dag\", schedule_interval=\"@daily\")\n",
    "\n",
    "    input_list = [\"item1\", \"item2\", \"item3\"]\n",
    "\n",
    "    for item in input_list:\n",
    "        task = PythonOperator(\n",
    "            task_id=f\"process_item_{item}\",\n",
    "            python_callable=process_list_item,\n",
    "            op_args=[item],\n",
    "            dag=dag,\n",
    "        )\n",
    "\n",
    "        task.set_upstream(None)\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        dag.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab8d78",
   "metadata": {},
   "source": [
    "# TOPIC: Sqoop\n",
    "1. Scenario: You want to import data from an Oracle database into Hadoop using Sqoop, but you only need to import specific columns from a specific table. Write a Sqoop command that performs the import, including the necessary arguments for column selection and table mapping.\n",
    "2. Scenario: You have a requirement to perform an incremental import of data from a MySQL database into Hadoop using Sqoop. Design a Sqoop command that imports only the new or updated records since the last import.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86b4c5d",
   "metadata": {},
   "source": [
    "**Scenario: You want to import data from an Oracle database into Hadoop using Sqoop, but you only need to import specific columns from a specific table. Write a Sqoop command that performs the import, including the necessary arguments for column selection and table mapping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26cb22",
   "metadata": {},
   "source": [
    "    sqoop import --connect jdbc:oracle:thin:@localhost:1521:xe \n",
    "                    --username user \n",
    "                    --password password \n",
    "                    --table table_name \n",
    "                    --columns column1,column2,column3 \n",
    "                    --target-dir /user/hadoop/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126230ac",
   "metadata": {},
   "source": [
    "**Scenario: You have a requirement to perform an incremental import of data from a MySQL database into Hadoop using Sqoop. Design a Sqoop command that imports only the new or updated records since the last import.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72cda6",
   "metadata": {},
   "source": [
    "    sqoop import --connect jdbc:mysql://localhost:3306/database\\\n",
    "                 --username user --password password \\\n",
    "                 --table table_name\\ \n",
    "                 --incremental lastmodified \\\n",
    "                 --last-value 2022-07-17T12:00:00Z \\\n",
    "                 --target-dir /user/hadoop/data\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9239f7",
   "metadata": {},
   "source": [
    "**3. Scenario: You need to export data from Hadoop to a Microsoft SQL Server database using Sqoop. Develop a Sqoop command that exports the data, considering factors like database connection details, table mapping, and appropriate data types.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a655b91",
   "metadata": {},
   "source": [
    "    sqoop export --connect jdbc:sqlserver://localhost:1433;databaseName=database \n",
    "                --username user --password password \n",
    "                --table table_name \n",
    "                --target-dir /user/hadoop/data \n",
    "                --fields-terminated-by '\\t' \n",
    "                --map-column-types string:VARCHAR,int:INT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11253636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c7672a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
